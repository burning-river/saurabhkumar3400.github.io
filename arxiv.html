<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Topic Modeling</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="css/clean-blog.min.css" rel="stylesheet">
<style>
table, th, td {
  border: 1px solid black;
    text-align: center;
}
    tr:nth-child(even) {
  background-color: #dddddd;
}
</style>
    
<style>
p {
  text-align: justify;
  text-justify: inter-word;
}
</style>
    
</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
     <a class="navbar-brand" href="index.html">Saurabh Kumar's Blog</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="index.html">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="about.html">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="contact.html">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Header -->
  <header class="masthead" style="background-image: url('./static/folder/arxiv/2020_cloud.jpg')">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="post-heading">
            <h1>Topic Modeling of Research Publications</h1>
            <h2 class="subheading">Viewing the research landscape</h2>
            <span class="meta">Posted on
              January 12, 2021</span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- Post Content -->
  <article>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
            <h2> About</h2>
         <p>
          This project is geared towards young researchers such as undergraduates motivated to pursue graduate school. Undergraduate research experience is a major factor in graduate admissions. However, as a beginner, it is extremely challenging for an undergrad to identify the broad research topics and the fastest growing areas in any field he/she is interested in. Moreover, simply going through publication databases isn't a viable solution: every year thousands of papers are published in every research area in physics!
         </p>
         <p>
            This project attempts to answer some of these problems. I have identified and compared the buzzwords of two different years which gives us an indication of the rising trends in physics. Using topic modelling, I have identified broad research topics as well as sub-topics contained in them. An interested reader can not only get a clear view of the research landscape in any broad are in physics but also find out some of the specific questions that belong to any field.
        </p>
            <p>Visit my <a href="https://github.com/sxk1031/arxiv_topic_modeling"><u>GitHub</u></a> repository to understand the machine learning techniques implemented in this project.
            </p>
            <h1> Exploratory Data Analysis </h1>
            
            <h2>Rising Trends in Physics</h2>
<p>Using the <a href="https://arxiv.org">arXiv</a> database for physics, I have compared the buzzwords in all areas of physics between two different years, 2010 (on the left) and 2020 (on the right). To create this comparison, I analyzed ~7400 publication titles from 2010 and ~17,000 from 2020. We can see some emerging trends such as 'neural networks' and 'machine learning' as well as some topics that have stayed both stable and prolific over the years such as 'optics'. This is an indication that machine learning is gaining popularity in physics and is a very useful skill to possess. Notice that <strong>COVID</strong> was also talked about frequently by physicists in 2020. 'Neural Network' can be seen as an emerging technique in 2020, unheard of ten years ago.</p>
    <div class="row justify-content-center">
        <!-- Wordcloud 2010-->
        <div class="col-sm-20 col-sm-offset-1 col-lg-6">
            <img src="./static/folder/arxiv/2010_cloud.jpg" alt="" style="width:125%" style="padding-bottom:5em;"/>
        </div>
        <!-- Wordcloud 2020-->
        <div class="col-sm-20 col-sm-offset-1 col-lg-6">
                <img src="./static/folder/arxiv/2020_cloud.jpg" alt="" style="width:125%"/>
        </div>
    </div>
<p> In 2010, the top 10 most used words were:
<br>
'dynamic',
 'quantum',
 'network',
 'optical',
 'plasma',
 'theory',
 'state',
 'simulation',
 'electron',
 'particle'
</p>
<p> In 2020, the top 10 most used words are:
<br>
'dynamic',
 'quantum',
 'network',
 'optical',
 'simulation',
 'flow',
 'COVID',
 'state',
 'Modeling',
 'Neural Network'
</p>
<h2>Most popular fields in Physics</h2>
<p>
A bar chart provided below compares the most active fields within physics between
2010 and 2020. The numbers on the X-axis are the publication count. Comparing the 2010 and 2020 research trends, we see that the field of 'Optics' stayed the most active in a decade. In contrast, 'Applied Physics' has become more active recently whereas 'Physics and Society' has become less popular since 2010.
</p>
    <div class="row justify-content-center">
        <!-- Wordcloud 2010-->
        <div class="col-sm-20 col-sm-offset-1 col-lg-6">
            <img src="./static/folder/arxiv/2010_important_areas.jpg" alt="" style="width:125%" style="padding-bottom:5em;"/>
        </div>
        <!-- Wordcloud 2020-->
        <div class="col-sm-20 col-sm-offset-1 col-lg-6">
                <img src="./static/folder/arxiv/2020_important_areas.jpg" alt="" style="width:140%"/>
        </div>
    </div>
            <br>
<p> Also notice that even in a given field, for example Optics, the total number of publications has grown almost by a factor of three. Physics as an entire discipline is growing with every decade, more papers being published every year.  
            </p>            
<h2>Rising trends: a closer look</h2>
            <p>
We can also look at the change in popularity of some key words/phrases over the years. Plotted below are the frequency vs. year (on the left in blue) and percentage increase from the previous year vs. year (on the right in red) of two phrases, such as 'Neural Network' (top panel) and 'Black Hole' (bottom panel) for example. Neural networks has seen a steady increase in popularity over the years, especially in the year 2017. Black holes, on the other hand, gained popularity between 2015 and 2017 which could be due to the discovery of binary black hole mergers by LIGO in 2015.
            </p>
<br>
  <div class="row">
        <div class="col-sm-20 col-sm-offset-1 col-lg-6">
            <img src="./static/folder/arxiv/trend_nn_freq.jpg" alt="" style="width:110%" style="padding-bottom:5em;"/>
      </div>

  <div class="col-sm-20 col-sm-offset-1 col-lg-6">
            <img src="./static/folder/arxiv/trend_nn_inc_pct.jpg" alt="" style="width:110%" style="padding-bottom:5em;"/>
      </div>
     </div>
<br>
  <div class="row">
        <div class="col-sm-20 col-sm-offset-1 col-lg-6">
            <img src="./static/folder/arxiv/trend_bhole_freq.jpg" alt="" style="width:110%" style="padding-bottom:5em;"/>
      </div>

  <div class="col-sm-20 col-sm-offset-1 col-lg-6">
            <img src="./static/folder/arxiv/trend_bhole_inc_pct.jpg" alt="" style="width:110%" style="padding-bottom:5em;"/>
      </div>
    </div>
<br>
            <h2> Concluding Remarks</h2>
            So far we looked at some Exploratory Data Analysis results.
            <ol> 
                <li> We identified some new techniques such as 'Neural Networks' that are gaining traction in physics. For a motivated undergrad, having some experience in such techniques could play a vital role in graduate admissions decisions. </li>
                <li> One could also analyze the change in popularity of certain terms over the years. For instance, 'Black Holes' suddenly started to trend in 2015. Identifying such suddenly rising trends could point to some groundbreaking discoveries/publications worth being aware about. </li>
                <li> Finally, one could examine which are the most stable fields over the years, perhaps indicating that the field is vibrant and full of mysteries.</li>   
            </ol>
            
<h1>Topic modeling </h1>
<p>
    For topic identification, I implemented unsupervised machine learning algorithms such as <i>k</i>-means clustering and the Latent Dirichlet Allocation or LDA. In <i>k</i>-means clustering, each document is viewed as a multi-dimensional vector of the matrix. Two documents are similar to one another if their corresponding vectors are close to one another or, in other words, if the difference of their vectors is small in length. We can use this common clustering method to group our documents together to find topics of research in Astophysics (astro-ph). 
    </p>
    <p>
    In LDA, each word in a document is assigned a topic. Initially, this assignment is done randomly. To improve upon the assignment of topics to each word, we look at the probability of a word, <i>w</i>, belonging to a particular topic, <i>t</i>. This is calculated by making a subsample of all documents that are classified as a particular topic <i>t</i> and the number of documents in that subsample that contains the word <i>w</i>. The LDA then goes through each document in the corpus and calculates the probability that a topic <i>t</i> is contained in a document <i>d</i>. These two probabilities assign a new probability to every word in the document of belonging to a topic. Repeating this process many times gives us an equilibrium value of the probabilities associated with each word and document.
</p>
<p>         
    Let us first begin with the simplest clustering algorithm, the <i>k</i>-means. After cleaning up the data, the <i>k</i>-means clusters can be labelled as:
</p>           
  <div class="row">
        <div class="col-sm-20 col-sm-offset-1 col-lg-6">
<table >
  <tr>
    <th><i>k</i>-means clustering</th>
  </tr>
  <tr>
<td>Stellar & Planetary physics</td>
</tr>
<tr>    
<td>Solar physics</td>
</tr>
<tr>    
<td>Gravitational waves and Neutron stars</td>
</tr>
<tr>    
<td>Gravitational waves and Black holes</td>
</tr>
<tr>    
<td>Dark Matter</td>
</tr>
</table>
</div>
    </div>
<br>
            <h2> Visualizations</h2> 
            <p> Let's look at a few visualizations from the clustering algorithms to get more insights from the data.</p>
            <h3> Word Cloud </h3>
            <p>First let's take a look at the most frequent words in a couple of the clusters through wordclouds. This will help us label the 5 clusters.</p>
            <img src="./static/folder/arxiv/kmc_wordcloud.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            <p> The two wordclouds above belong to the clusters from the <i>k</i>-means algorithm. Similar wordclouds can be made for all the other clusters for both algorithms. More details have been provided in the Python notebook in GitHub.</p>
            <h3> Cluster size </h3>
            <img src="./static/folder/arxiv/k_means_cluster_size.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            <p>
                We see that in the <i>k</i>-means approach, <strong>Stellar & Planetary physics</strong> is the largest cluster. This seems to be consistent with my personal experience of going through the arXiv database in the past few years. 
            </p>    
            <h3> Word Frequencies </h3>
            <p> Next, we can look at the number of occurences in each cluster of some of the most frequent words in the corpus. This visualization could suggest some additional stop words that we might want to add (and then reprocess the resulting new bag of words matrix) to make our clustering better. </p>
            <img src="./static/folder/arxiv/k_means_word_freq_1.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            <img src="./static/folder/arxiv/k_means_word_freq_2.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            
            <h3> Number of Clusters </h3>
            <p> To verify that 5 is a reasonable assumption of the number of clusters, let's look at the <i>inertia</i> plot. The inertia value on the y-axis is related to the average distance between the cluster center and the data points. The lower the inertia the better the number of clusters.</p>
            <img src="./static/folder/arxiv/k_means_cluster_number.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            <p>One can notice that the inertia marginally tapers at 4. The improvements in inertia become a little gradual beyond that value. Therfore, 5 is a good choice for the number of clusters.</p>
            
            <h3> Low-dimensional representation </h3>
            <p> We can use PCA to project the high-dimensional vector representation of our text corpus into a lower-dimensional space. This could give us an indication of the separation or overlapping between the clusters.</p>
            <img src="./static/folder/arxiv/pca.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            <p> We see that some clusters, eg. Cluster 3 (brown) and 4 (gray) are relatively well-defined: most of the documents belonging to these clusters lie along one axis. There is some mixing in the other clusters, e.g. Clusters 0 (blue) and 2 (green), possibly because of the limitation of the projection down into a two-dimensional space. Alternatively, this could also mean that there is overlap between the clusters. There is reason that this might be true because neutron stars are the end state of many stars. Therefore Cluster 0 (Stellar & Planetary physics) and Cluster 2 (Gravitational Waves and Neutron stars) are related.</p>
                    
            <h2> Latent Dirichlet Allocation or LDA </h2>
            <p> One way to improve on our current clustering method is to predict the probability with which a document is assigned a cluster. LDA is one such algorithm which does that. After pre-processing the data and implementing the LDA algorithm from the <code>gensim</code> library, I arrived at the following topics:</p>
              <div class="row">
        <div class="col-sm-20 col-sm-offset-1 col-lg-6">
<table >
  <tr>
    <th>LDA topics</th>
  </tr>
  <tr>
<td>Observations from Hubble Space Telescope</td>
</tr>
<tr>    
<td>Galactic center</td>
</tr>
<tr>    
<td>Dark Matter & Baryons</td>
</tr>
<tr>    
<td>Stellar Physics</td>
</tr>
<tr>    
<td>Galaxy surveys & galactic physics</td>
</tr>
<tr>    
<td>Gravitational wave sources</td>
</tr>
<tr>    
<td>Cosmic Microwave Background or CMB</td>
</tr>
</table>
</div>
    </div>
            <p> We will see in a moment why 7 is the appropriate number of clusters for LDA.</p>
            <h3> Cluster Size</h3>
            For the LDA, the cluster/topic sizes were:
            <img src="./static/folder/arxiv/lda_cluster_size.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            
            <h3> Word Frequencies </h3>
            <img src="./static/folder/arxiv/lda_word_freq_1.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            
            <h3> Number of Topics: Coherence Score </h3>
            <p> The coherence score captures the semantic information of the document by looking at the co-occurence of pairs of words in the document. It can take a value between -1 (worst) and 1 (best). I calculated the coherence scores for different number of topics. I then plotted the percentage improvement in the coherence score with increasing number of topics: </p>
            <img src="./static/folder/arxiv/lda_coherence_improv.jpg" alt="" style="width:120%" style="padding-bottom:5em;"/>
            <p> The biggest improvement in coherence score, 15%, occurs when the number of topics increases to four. However, from my experience, it seems like a low number. The next biggest improvement occurs when there are 7 topics. Therefore, the initial assumption of 7 topics turns out to be reasonable.</p>       
            
            <h2> Example </h2>
                <p> Finally, let us take a look at an abstract/document from the database/corpus and compare the topics it was assigned to by the two different methods. </p>
                <p><mark> We present an upgrade of the OGLE Collection of RR Lyrae stars in the
                    Galactic bulge and disk. The size of our sample has been doubled and reached 78
                    350 RR Lyr variables, of which 56 508 are fundamental-mode pulsators (RRab
                    stars), 21 321 pulsate solely in the first-overtone (RRc stars), 458 are
                    classical double-mode pulsators (RRd stars), and 63 are anomalous RRd variables
                    (including five triple-mode pulsators). For all the newly identified RR Lyr
                    stars, we publish time-series photometry obtained during the OGLE Galaxy
                    Variability Survey.
                    We present the spatial distribution of RR Lyr stars on the sky, provide a
                    list of globular clusters hosting RR Lyr variables, and discuss the Petersen
                    diagram for multimode pulsators. We find new RRd stars belonging to a compact
                    group in the Petersen diagram (with period ratios P_1O/P_F = 0.74 and
                    fundamental-mode periods P_F = 0.44 d) and we show that their spatial
                                        distribution is roughly spherically symmetrical around the Milky Way center. </mark></p>
            
            <p> The <i>k</i>-means clustering method assigns the document to the <i>Stellar & Planetary physics</i> cluster. This is a fair result given that the abstract contains the term 'stars'. </p>
            
            <p> The one shortcoming of the <i>k</i>-means clustering method is that it strictly categorizes the documents. LDA is advantageous since it gives us the probabilities of the topics for each document. For the above abstract, the 2 top-most topics are <i>Galactic center</i> with probability of 0.75 and <i>Stellar Physics</i> with probability of 0.08.</p>
            <h2> Concluding Remarks </h2>
            <ol> 
                <li> We saw that in a broad field such as Astrophysics, there are some distinct sub topics for example: stellar and planetary physics, gravitational waves, dark matter and cosmic microwave background. </li> 
                
                <li> For the <i>k</i>-means clustering, 5 is an appropriate number of clusters. For LDA, which takes into account the semantic structure of the text, the appropriate number of topics is 7.
                
                <li> We can also look at the multidisciplinary nature of the articles through LDA. </li>
            </ol>
      </div>
    </div>         
  </article>
    
  <hr>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            <li class="list-inline-item">
              <a href="https://github.com/sxk1031/arxiv_topic_modeling">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
              
            <li class="list-inline-item">
              <a href="https://www.linkedin.com/in/saurabhkumar3400/">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>  
              
          </ul>
          <p class="copyright text-muted">Copyright &copy; Saurabh Kumar's Blog 2021</p>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/clean-blog.min.js"></script>

</body>

</html>
